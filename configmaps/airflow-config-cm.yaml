apiVersion: v1
data:
  airflow.cfg: |
    [core]
    airflow_home = /usr/local/airflow
    dags_folder = /usr/local/airflow/dags/repo/airflow/example_dags
    base_log_folder = /usr/local/airflow/logs
    executor = KubernetesExecutor
    plugins_folder = /usr/local/airflow/plugins
    sql_alchemy_conn = $sql_alchemy_conn

    [scheduler]
    child_process_log_directory = /usr/local/airflow/logs/scheduler

    [webserver]
    rbac = false

    [kubernetes]
    airflow_configmap = airflow-config
    worker_container_repository = airflow
    worker_container_tag = latest
    worker_container_image_pull_policy = IfNotPresent
    worker_dags_folder = /usr/local/airflow/dags
    delete_worker_pods = true
    git_repo = https://github.com/apache/airflow
    git_branch = master
    git_subpath = airflow/example_dags
    git_dags_folder_mount_point = /usr/local/airflow/dags
    logs_volume_claim = airflow-logs
    logs_volume_subpath =

    in_cluster = true
    namespace = airflow

    # for cloning dags from git repositories into volumes: https://github.com/kubernetes/git-sync
    git_sync_container_repository = k8s.gcr.io/git-sync
    git_sync_container_tag = v3.0.1
    git_sync_init_container_name = git-sync-clone

    [kubernetes_node_selectors]
    # the key-value pairs to be given to worker pods.
    # the worker pods will be scheduled to the nodes of the specified key-value pairs.
    # should be supplied in the format: key = value

    [kubernetes_secrets]
    AIRFLOW__CORE__SQL_ALCHEMY_CONN = airflow-env=AIRFLOW__CORE__SQL_ALCHEMY_CONN
    AIRFLOW_HOME = airflow-env=AIRFLOW_HOME

    [cli]
    api_client = airflow.api.client.json_client
    endpoint_url = https://airflow.crunchanalytics.cloud

    [api]
    auth_backend = airflow.api.auth.backend.default

    [admin]
    # ui to hide sensitive variable fields when set to true
    hide_sensitive_variable_fields = true
kind: ConfigMap
metadata:
  labels:
    app: airflow
  name: airflow-config
  namespace: airflow
